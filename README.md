![](docs/images/halitosis_banner.png)

# HALitosis Attack Framework

[![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white)](https://www.tensorflow.org/)
[![Discord](https://img.shields.io/badge/%3CHALitosis%3E-%237289DA.svg?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/KgrChecyQt)
[<img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />]("./LICENSE.md")

<img align="right" src="docs/images/halitosis_badge_clear.png" height="300px">

> HALitosis is a framework for machine learning attacks

[![Coverage Status](https://coveralls.io/repos/github/Man1f0ld/HALitosis/badge.svg?branch=main)](https://coveralls.io/github/Man1f0ld/HALitosis?branch=main)

Machine Learning algorithms are used to make state of the art predictions from large, complicated datasets - however, their complexity create vulnerabilities that allow a saavy attacker to install backdoors that when triggered can create specific outputs that can even contain their own logic.

HALitosis is a [Python](https://www.python.org/) library that catalogues and categorizes these various methods of attack for ML algorithms written in [Tensorflow](https://www.tensorflow.org/), these include:

- Data Poisoning
- Training Poisoning
- RNG Bias Poisoning
- Backdoor Injection
- Meet-in-the-Middle Backdoors

## Getting Started

